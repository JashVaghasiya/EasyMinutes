{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfE5dceSp0a+epL5KyXBVG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JashVaghasiya/EasyMinutes-MOM/blob/main/Action_words_Rephraser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers nltk torach numpy pandas"
      ],
      "metadata": {
        "id": "cmaHSDn2UB78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "svlny_SJYrqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "0I9rFg9zUOjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration\n",
        "import nltk\n",
        "\n",
        "# Make sure to download necessary NLTK data and spaCy model\n",
        "nltk.download('punkt', quiet=True)\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "# Load spaCy for NLP tasks\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load BERT and T5 models and tokenizers\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming 'folder_path' contains the BERT model and tokenizer\n",
        "folder_path = '/content/drive/MyDrive/my_model'  # Update this path\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(folder_path)\n",
        "bert_model = BertForSequenceClassification.from_pretrained(folder_path).to(device)\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
        "\n",
        "def split_into_sentences(transcript):\n",
        "    \"\"\"Split transcript into sentences using NLTK.\"\"\"\n",
        "    return nltk.tokenize.sent_tokenize(transcript)\n",
        "\n",
        "def rephrase_with_model(sentence, model, tokenizer, device):\n",
        "    \"\"\"Rephrase a sentence using the T5 model.\"\"\"\n",
        "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def extract_and_rephrase_action_phrases(sentence, nlp, t5_model, t5_tokenizer, device):\n",
        "    \"\"\"Extract action phrases from a sentence and rephrase them for clarity.\"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    action_phrase = \"\"\n",
        "\n",
        "    for token in doc:\n",
        "        # Check for the verb that indicates an action\n",
        "        if token.pos_ == \"VERB\":\n",
        "            # Construct a basic phrase: Verb + Direct Object\n",
        "            direct_objects = [child for child in token.children if child.dep_ == \"dobj\"]\n",
        "            if direct_objects:\n",
        "                action_phrase = token.text + \" \" + \" \".join([dobj.text for dobj in direct_objects])\n",
        "            else:\n",
        "                # If no direct object, use the verb itself\n",
        "                action_phrase = token.text\n",
        "            break  # Focus on the first action verb found for simplicity\n",
        "\n",
        "    # Rephrase the constructed action phrase for better clarity, if any phrase was constructed\n",
        "    if action_phrase:\n",
        "        # Correcting the model usage with proper input formatting\n",
        "        input_ids = t5_tokenizer.encode(action_phrase, return_tensors=\"pt\").to(device)\n",
        "        outputs = t5_model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
        "        rephrased_phrase = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return rephrased_phrase\n",
        "    else:\n",
        "        return \"\"  # Return an empty string if no action verb was identified or no phrase could be constructed\n",
        "\n",
        "def process_transcript_and_extract_refined_phrases(transcript, bert_model, bert_tokenizer, t5_model, t5_tokenizer, nlp, device):\n",
        "    sentences = split_into_sentences(transcript)\n",
        "    refined_phrases = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        inputs = bert_tokenizer.encode_plus(sentence, return_tensors=\"pt\").to(device)\n",
        "        outputs = bert_model(**inputs)  # Corrected line: inputs are unpacked as a dictionary\n",
        "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "        if predicted_class == 1:  # Action sentence\n",
        "            refined_phrase = extract_and_rephrase_action_phrases(sentence, nlp, t5_model, t5_tokenizer, device)\n",
        "            if refined_phrase:\n",
        "                refined_phrases.append(refined_phrase)\n",
        "\n",
        "    return refined_phrases\n",
        "\n",
        "# Example usage\n",
        "transcript = \"Jane: Good morning, everyone. Let’s start with the current status of our software and hardware updates for our next-gen universal remote. Alex, can you kick us off with the software update? Alex: Sure, Jane. The team has successfully integrated the latest voice recognition software. However, we're encountering latency issues when the remote processes multiple commands in quick succession. We’re considering optimizing the command queue management to resolve this.  Sam: On the hardware side, we’ve upgraded the microcontroller to support the new software features Alex mentioned. We're also experimenting with a new battery design to extend the remote’s life. The prototype is promising, but we need to ensure it doesn't significantly increase the production cost. Mia: Design-wise, we’ve incorporated feedback from the last user testing session. The new button layout and the tactile feedback have been well-received in preliminary tests. However, aligning the new design with Sam’s hardware changes is our next challenge. Eric: From a QA perspective, we need to schedule a comprehensive testing phase for both the software updates and the new hardware components. It’s crucial to test the voice recognition in various environments to ensure it performs consistently. Jane: Great updates, team. Addressing the latency in voice command processing is our top priority. Alex, let’s brainstorm with your team on the command queue management. Sam, keep us posted on the battery design cost analysis. Mia, work with Sam to ensure the design and hardware are aligned. Eric, start planning the testing phase, focusing on voice recognition performance. Alex: Will do. I’ll also look into leveraging cloud processing to reduce the load on the remote’s processor, which might help with the latency. Sam: Understood. I’ll coordinate with the suppliers to get an estimate on the new battery costs and report back by next week. Mia: I’ll schedule a meeting with Sam tomorrow to discuss the design and hardware integration. Eric: I’m on it. I’ll prepare a detailed testing plan and reach out to the team for input. Jane: Excellent. Let’s reconvene next week to review progress on these action items. Thank you, everyone, for your hard work. Meeting adjourned.\"\n",
        "refined_action_phrases = process_transcript_and_extract_refined_phrases(\n",
        "    transcript, bert_model, bert_tokenizer, t5_model, t5_tokenizer, nlp, device)\n",
        "print(\"Refined Action Phrases:\", refined_action_phrases)\n"
      ],
      "metadata": {
        "id": "0OXDsFjxULV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_repetitive_words(phrase):\n",
        "    words = phrase.split()\n",
        "    # Remove consecutive duplicate words\n",
        "    filtered_words = [word for i, word in enumerate(words) if i == 0 or word != words[i-1]]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def refine_action_phrases(phrases):\n",
        "    # First, remove repetitive words within each phrase\n",
        "    refined_phrases = [remove_repetitive_words(phrase) for phrase in phrases]\n",
        "\n",
        "    # Then, filter out phrases that are too generic or do not imply a clear action\n",
        "    # Note: This step might require manual adjustment based on your criteria for \"actionable\" phrases\n",
        "    actionable_phrases = [phrase for phrase in refined_phrases if len(phrase.split()) > 1 and not phrase.lower() in ['work', 'start', 'look at']]\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    seen = set()\n",
        "    deduplicated_phrases = [x for x in actionable_phrases if not (x in seen or seen.add(x))]\n",
        "    return deduplicated_phrases\n",
        "\n",
        "refined_action_phrases = refine_action_phrases(refined_action_phrases)\n",
        "print(refined_action_phrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPTCprmlVayj",
        "outputId": "a9078bb6-982d-48ff-e460-a6391edfc0a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['kick us', 'integrated software', 'Microcontroller upgraded microcontroller', 'incorporated feedback', 'aligning design', 'Be besoin', 'addressing latency', 'keep us on us.', 'schedule meeting', 'préparer plan.', 'Vielen Dank für']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_present_continuous(phrase):\n",
        "    doc = nlp(phrase)\n",
        "    new_phrase = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Check if the token is a verb\n",
        "        if token.pos_ == \"VERB\":\n",
        "            # Convert verb to -ing form\n",
        "            ing_verb = token.lemma_ + \"ing\"\n",
        "            new_phrase.append(ing_verb)\n",
        "        else:\n",
        "            new_phrase.append(token.text)\n",
        "\n",
        "    return ' '.join(new_phrase)\n",
        "\n",
        "present_continuous_phrases = [to_present_continuous(phrase) for phrase in refined_action_phrases]\n",
        "print(present_continuous_phrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTejWwm0Uxkn",
        "outputId": "947126a6-73f0-4309-cbb9-069413e20116"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['kicking us', 'integrateing software', 'Microcontroller upgradeing microcontroller', 'incorporateing feedback', 'aligning design', 'Be besoin', 'addressing latency', 'keeping us on us .', 'schedule meeting', 'préparer plan .', 'Vielen Dank für']\n"
          ]
        }
      ]
    }
  ]
}